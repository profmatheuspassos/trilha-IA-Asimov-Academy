{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224fe8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_temperatura_atual(local, unidade=\"celsius\"):\n",
    "    # Função que retorna a temperatura de uma cidade com base no nome da cidade (local)\n",
    "    # e na unidade de medida (celsius ou fahrenheit). A temperatura é simulada.\n",
    "    \n",
    "    if \"são paulo\" in local.lower():\n",
    "        # Se o local for \"São Paulo\" (insensível a maiúsculas/minúsculas), retorna uma\n",
    "        # temperatura simulada de 32 graus na unidade escolhida.\n",
    "        return json.dumps(\n",
    "            {\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "        )\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        # Se o local for \"Porto Alegre\", retorna uma temperatura simulada de 25 graus.\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade}\n",
    "        )\n",
    "    elif \"rio de janeiro\" in local.lower():\n",
    "        # Se o local for \"Rio de Janeiro\", retorna uma temperatura simulada de 35 graus.\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Rio de Janeiro\", \"temperatura\": \"35\", \"unidade\": unidade}\n",
    "        )\n",
    "    else:\n",
    "        # Para qualquer outra cidade, retorna \"unknown\" como temperatura,\n",
    "        # indicando que a temperatura não está disponível.\n",
    "        return json.dumps(\n",
    "            {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "        )\n",
    "\n",
    "\n",
    "# Lista de ferramentas (tools) disponíveis para o modelo usar, definindo uma função\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",  # Indica que o tipo de ferramenta é uma função\n",
    "        \"function\": {\n",
    "            \"name\": \"obter_temperatura_atual\",  # Nome da função que será usada\n",
    "            \"description\": \"Obtém a temperatura atual em uma dada cidade\",  # Descrição da função\n",
    "            \"parameters\": {  # Define os parâmetros que a função precisa para funcionar\n",
    "                \"type\": \"object\",  # O tipo de parâmetro é um objeto (JSON)\n",
    "                \"properties\": {\n",
    "                    \"local\": {  # Primeiro parâmetro, o nome da cidade\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"O nome da cidade. Ex: São Paulo\",  # Explica que a função precisa do nome da cidade\n",
    "                    },\n",
    "                    \"unidade\": {  # Segundo parâmetro, a unidade de temperatura\n",
    "                        \"type\": \"string\", \n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]  # A unidade pode ser celsius ou fahrenheit\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"local\"],  # O parâmetro \"local\" é obrigatório\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Dicionário que relaciona os nomes das funções disponíveis com suas implementações\n",
    "funcoes_disponiveis = {\n",
    "    \"obter_temperatura_atual\": obter_temperatura_atual,  # Relaciona o nome da função com a função definida anteriormente\n",
    "}\n",
    "\n",
    "# Lista de mensagens, que simula uma interação entre o usuário e o assistente\n",
    "mensagens = [\n",
    "    {\n",
    "        \"role\": \"user\",  # Papel do usuário no chat\n",
    "        \"content\": \"Qual é a temperatura em São Paulo, Porto Alegre e Brasília?\"  # Pergunta do usuário\n",
    "    }\n",
    "]\n",
    "\n",
    "# Chama a API da OpenAI para gerar uma resposta com base no modelo especificado\n",
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # Modelo utilizado para gerar a resposta\n",
    "    messages=mensagens,  # Mensagens trocadas até o momento\n",
    "    tools=tools,  # Ferramentas disponíveis para o modelo usar\n",
    "    tool_choice=\"auto\",  # O modelo escolhe automaticamente a ferramenta a ser usada\n",
    ")\n",
    "\n",
    "# Extrai a mensagem gerada pelo modelo\n",
    "mensagem_resp = resposta.choices[0].message\n",
    "# Extrai as chamadas de ferramentas feitas pelo modelo, se houver\n",
    "tool_calls = mensagem_resp.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    # Se houver chamadas de ferramentas, adiciona a resposta à lista de mensagens\n",
    "    mensagens.append(mensagem_resp)\n",
    "    for tool_call in tool_calls:\n",
    "        # Itera sobre as chamadas de ferramentas para executar as funções associadas\n",
    "        function_name = tool_call.function.name  # Extrai o nome da função chamada\n",
    "        function_to_call = funcoes_disponiveis[function_name]  # Pega a função correspondente\n",
    "        function_args = json.loads(tool_call.function.arguments)  # Converte os argumentos da função de JSON para um dicionário\n",
    "        # Chama a função com os argumentos fornecidos pelo modelo\n",
    "        function_response = function_to_call(\n",
    "            local=function_args.get(\"local\"),  # Obtém o parâmetro 'local' da função\n",
    "            unidade=function_args.get(\"unidade\"),  # Obtém o parâmetro 'unidade' da função\n",
    "        )\n",
    "        # Adiciona a resposta da ferramenta (função) à lista de mensagens\n",
    "        mensagens.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id,  # Identificador da chamada da ferramenta\n",
    "                \"role\": \"tool\",  # Papel como ferramenta (função)\n",
    "                \"name\": function_name,  # Nome da função chamada\n",
    "                \"content\": function_response,  # Resultado da execução da função\n",
    "            }\n",
    "        )\n",
    "    # Após as chamadas de ferramentas, gera uma nova resposta com base nas mensagens atualizadas\n",
    "    segunda_resposta = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Modelo utilizado para gerar a segunda resposta\n",
    "        messages=mensagens,  # Mensagens atualizadas com as respostas das funções\n",
    "    )\n",
    "\n",
    "# Extrai a resposta final gerada pelo modelo\n",
    "mensagem_resp = segunda_resposta.choices[0].message\n",
    "# Imprime o conteúdo da resposta final\n",
    "print(mensagem_resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a7bea",
   "metadata": {},
   "source": [
    "#### Explicação:\n",
    "\n",
    "* O script define uma função chamada `obter_temperatura_atual`, que simula a obtenção de temperaturas para algumas cidades específicas.\n",
    "\n",
    "* Um conjunto de ferramentas (funções) é disponibilizado para um modelo de linguagem da OpenAI. Essas ferramentas podem ser chamadas automaticamente pelo modelo, dependendo do que o usuário perguntar.\n",
    "\n",
    "* O script utiliza a API da OpenAI para processar uma interação do usuário, onde o modelo tenta responder à pergunta sobre a temperatura em São Paulo e Porto Alegre. Se necessário, o modelo chama as funções definidas para fornecer uma resposta mais precisa.\n",
    "\n",
    "* Após o modelo utilizar as ferramentas e atualizar as mensagens, ele gera uma resposta final, que é exibida no console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a0ba2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61236ae",
   "metadata": {},
   "source": [
    "Vou explicar passo a passo o que ocorre nesse script:\n",
    "\n",
    "1. **Definição da Função `obter_temperatura_atual`**:\n",
    "   - A função `obter_temperatura_atual` é responsável por retornar a temperatura simulada de uma cidade com base no nome do local passado como parâmetro e na unidade de medida (`celsius` ou `fahrenheit`).\n",
    "   - Para as cidades \"São Paulo\", \"Porto Alegre\" e \"Rio de Janeiro\", retorna-se uma temperatura fixa simulada (32°C, 25°C e 35°C, respectivamente). Para qualquer outra cidade, retorna \"unknown\", indicando que a temperatura não está disponível.\n",
    "\n",
    "2. **Bloco `tools`**:\n",
    "   - O script define uma lista chamada `tools`, que contém informações sobre as ferramentas disponíveis para o modelo de linguagem usar. Aqui, a ferramenta é uma função chamada `obter_temperatura_atual`.\n",
    "   - A estrutura indica que a função requer dois parâmetros: `local` (nome da cidade) e `unidade` (unidade de temperatura). O parâmetro `local` é obrigatório.\n",
    "\n",
    "3. **Dicionário `funcoes_disponiveis`**:\n",
    "   - Este dicionário relaciona o nome da função (`obter_temperatura_atual`) com sua implementação, que é a própria função definida anteriormente. Esse dicionário será usado posteriormente para chamar a função correspondente.\n",
    "\n",
    "4. **Bloco `mensagens`**:\n",
    "   - A lista `mensagens` simula uma troca de mensagens em um contexto de chat, onde o usuário pergunta \"Qual é a temperatura em São Paulo e Porto Alegre?\". Essa mensagem será passada ao modelo.\n",
    "\n",
    "5. **Chamada da API do modelo**:\n",
    "   - O script faz uma chamada à API da OpenAI, especificando o modelo a ser utilizado (`gpt-4o-mini`) para gerar uma resposta com base nas mensagens anteriores.\n",
    "   - As ferramentas disponíveis (`tools`) são fornecidas para que o modelo possa escolher qual ferramenta usar.\n",
    "   - A opção `\"tool_choice\": \"auto\"` permite que o modelo selecione automaticamente a ferramenta apropriada para responder à pergunta do usuário.\n",
    "\n",
    "6. **Extração da resposta gerada**:\n",
    "   - A resposta gerada pelo modelo é armazenada na variável `mensagem_resp`.\n",
    "   - Em seguida, são verificadas se há chamadas de ferramentas (funções) na resposta. Se houver, o código irá lidar com elas.\n",
    "\n",
    "7. **Execução das funções chamadas**:\n",
    "   - Se o modelo solicitou a execução de alguma ferramenta (função), o código entra em um loop que percorre todas as chamadas de funções realizadas pelo modelo.\n",
    "   - Para cada chamada de função, ele extrai o nome da função e os argumentos fornecidos (que estão em formato JSON).\n",
    "   - A função correspondente é chamada com os argumentos fornecidos, e o resultado dessa função é adicionado à lista de mensagens.\n",
    "\n",
    "8. **Nova resposta após a execução das funções**:\n",
    "   - Após as funções serem executadas, o script faz uma nova chamada à API da OpenAI, desta vez utilizando o modelo `gpt-3.5-turbo-0125`, e inclui as novas mensagens com os resultados das funções.\n",
    "   - A resposta final do modelo é extraída e impressa.\n",
    "\n",
    "Em resumo, o código simula uma conversa onde o usuário pergunta sobre a temperatura em São Paulo e Porto Alegre, o modelo de linguagem escolhe e chama a função `obter_temperatura_atual`, que retorna as temperaturas simuladas dessas cidades. Em seguida, a resposta final é impressa com base nas informações retornadas pelas funções."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
